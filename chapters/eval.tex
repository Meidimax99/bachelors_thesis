\chapter{Evaluation}
\label{chap:eval}

% -------------------------------------------------------------------------------------------------
%                                             Structure
% -------------------------------------------------------------------------------------------------



% Qualitative Analysis (Does not need a heading, the whole chapter is about this)

% - Cost of the design
%     - Context Switch (Just general cost, specifics later)
%         jacobVirtualMemoryContemporary1998 -> Pipeline flushing etc
%     - Exception Handler
%     - Optimization Opportunities
%         - Heisers Fast TLB Miss handler
%         - Inlining

% - General problems of a Segmented Design

% - More shortcommings related to the functional requirements of virtual memory
%     (Resolve shortcommings?)

% - Qualitative Cost analysis
%     - Problems with acquiring quantitative data from QEMU
%         - Typical Metrics (and if they can be acquired from qemu)
%             - Instruction Count
%             - Cycle count
%         - Real hardware: Comparing Cycle count, or measuring ms (-> MMU freezes the CPU)
%     - Mem Access Counting (Instruction counting?)

% - Smoothing out the implementation
% - handler inlining
% - Better control using MIPS TLB instructions
% -------------------------------------------------------------------------------------------------
%                                             Intro
% -------------------------------------------------------------------------------------------------

% Successful poc
The previous chapters show the successful and transparent replacement of the xv6 virtual memory system with a mapping function based TLB miss handler. However, the design and implementation have a number of shortcomings.
This chapter will discuss these shortcomings of the presented mapping function in light of typical requirements to virtual memory systems. It will also attempt to qualitatively analyze the cost of the exception handler implementing the mapping function and compare it to conventional page table based schemes. The chapter will also discuss some shortcomings of the implementation and give some ideas on how to alleviate the problems.
% The points will also shortly address some improvement ideas

% -------------------------------------------------------------------------------------------------
%                                             Software-Managed TLBs
% -------------------------------------------------------------------------------------------------
\section{Software-managed TLBs}
Managing TLBs in software grants a lot of flexibility, but this flexibility does not come
for free. While the general trade-offs between software and hardware TLB managements have
already been discussed in the Fundamentals chapter \ref{chap:fund}, this section will
go into more detail of the trade-offs of this specific design and implementation.

\subsection{Context Switch} The fast TLB miss handler presented by Gernot Heiser for the
L4/MIPS implementation only uses three registers: Two of those registers are reserved
for the kernel and the other one has to be saved in memory to later be restored \cite{heiserAnatomyHighPerformanceMicrokernel}.
While the processor still has to run the instructions, this is only minimally invasive
for the state of the kernel.

This implementation performs a complete save of the processor state, including most of the
32 registers that RISC-V offers.
This allows running C code from the exception handler without having to think about what
registers are mangled by the code, which is very useful for trying different implementations.
The trade-off here lies between ease of programmability and performance.

\subsection{Pipeline Flush} Exception handlers not only change the state of the processor that is
exposed to the programmer, they also disturb the transparent state consisting of the
instruction pipeline and the reorder buffer.
Running instructions from the context of the exception handler (coupled with RISC-V handling
memory faults precisely \cite{RISCVInstructionSet}) requires a flush of both reorder buffer
and instruction pipeline \cite{jacobVirtualMemoryContemporary1998}.
Instructions that have already been partially executed in the pipeline have to be restarted,
imposing extra cost on the software handler \cite{jacob1998look}.

\subsection{Exception Handler} The key difference of this exception handler and exception handlers
of other software-based virtual memory designs is that this one does not need to access
memory to generate mappings (apart from the register state save).
Looking back at listing \ref{lst:handler} it is apparent that there still are some
memory references:
Mostly the function call of the \texttt{get\_mapping()} function would generate code that
writes to the stack. But the function call is only for readability of the code and could also
be inlined.
Otherwise the implementation is based on calculation for determining the memory offset from
the ASID and bitwise operations.
This is an advantage over radix page table systems that may need up to five memory references
in contemporary architectures \cite{intel5LevelPaging5Level2017}.

% \todo{typical tlb replacement strategies}
% \todo{How does QEMU replace TLB entries -> index?} % Direct mapped
% With the new CSRs presented above, it is possible to write TLB entries.
% The placement of the entry and thus the entry to be replaced can not be selected.
% The replacement policy would be completely up to the hardware implementation.
% It would also be possible to further extend the ISA with additional CSRs to
% support specific selection of entries to be replaced, or to select between
% a number of replacement strategies.
% However, the efficiency of more complex strategies need to be weighed carefully
% against their benefits, since the TLB filling is on the critical path of all
% memory accesses.
% -------------------------------------------------------------------------------------------------
%                                             Segmented VM Desing (Design shortcommings)
% -------------------------------------------------------------------------------------------------
\section{Segmented Memory Design}
The segmented mapping function design has some fundamental problems that restrict the programs running on the system and the general flexibility of the system.

Programs can only have a \textbf{maximum size} which can (with this implementation) no be extended when the program requires more memory. This can happen at any system load and even if there is still a lot of unused memory in the address spaces of other programs.

This is due to the huge amount of \textbf{internal fragmentation} that this design introduces to the memory system. Even small programs take the maximum amount of memory the address space can offer.

In general, the memory system is very rigid concerning the size of programs and can only a fixed size. When working with such a system, the memory requirements of programs should be assessed beforehand and the address space size determined to best fit the systems needs.

Additionally, the selected maximum address space size also determines the amount of \textbf{multiprogramming} the system can handle. As every program needs a address space to run, the maximum number of processes the system can run at once is limited by the number of address spaces.
It is a trade-off between address space size and address space number.

A more refined implementation building on top of the segmented design may use overlays \cite{tanenbaumOS} to assign one ASID to multiple processes, swapping them in and out of memory as necessary. To increase the flexibility of the processes memory size, a way to assign multiple ASIDs to a single process may be implemented.

Further refining this idea of a many-to-many relationship between processes and address spaces boils down to conventional virtual memory systems operating on page granularity and allowing page sharing in processes.

Finally, the memory system does isolate processes from each other, but is not able to provide fine grained protection of logical program segments (like the text segment) from each other. As such, it is possible (maybe due to some malicious input) for the code to modify itself, corrupting the execution of the program.

%-------------------------------------------------------------------------------------------------
%                                             Functional Requirements
% -------------------------------------------------------------------------------------------------
\section{Memory System Requirements}
% Shortcommings of my implementation
For a quantitative assessment of the design, the functional requirements to virtual memory system from chapter \ref{chap:fund} are revisited.

\paragraph{Address Space Protection / Isolation} Since all memory accesses from virtual addresses go through the TLB, a proper isolation of processes in the TLB exception handler guarantees isolation of processes. ASIDs are used as the foundation of address calculation to provide each process with a distinct physical memory space.
Synchronization on the data structure managing ASIDs makes sure that no two processes can have the same ASID while both are alive.

\paragraph{Large Address Space} Segmentation restricts the maximum size each process can have. One could implement dynamic resizing of address spaces or allow processes to hold multiple ASIDs to allow processes to increase their memory over the statically determined maximum limit.
That would however increase the complexity of the mapping function. Another idea would be to have address spaces with different (predetermined) sizes that could then be assigned according to a programs memory demands.

\paragraph{Superpages} The current state of the design does not really deal in pages but rather in complete address spaces. As such, super pages are not supported.

\paragraph{Flexibility} Pages in the virtual memory space can be placed anywhere as long as they are within the maximum address space size. And they must not clash with the heap space that will always be expanding from low to high addresses.

\paragraph{Sparsity} Sparsity in page table based virtual memory systems is about efficiently
supporting huge address spaces with only a small numbers of pages being actually used. The bookkeeping should use as little additional memory as possible.

The stateless design requires no tables to store the bookkeeping for the mapping - there is no bookkeeping. However, huge address spaces are not supported and the internal fragmentation if very high.

\paragraph{Swapping} One of the most important tasks of a virtual memory system is to automate the swapping of memory pages between main memory and secondary storage. xv6 does not provide a good starting point to experiment with new virtual memory systems that also fulfil that requirement, as xv6 does not implement page swapping. Programs are completely held in memory and also loaded completely into memory on execution \cite{cox2011xv6}. As such, page table faults of any kind will result in xv6 killing the process.

% Was ist mit dem paging passiert -> Automatisches Tauschen von Seiten zwischen Haupt und Nebenspeicher?
% In xv6 nicht implementiert, könnte es mit dem Segmentierten Design implementiert werden

% TODO -> Perspektive richtung Embedded, wo isolation gewünscht aber kein Nebenspeicher existiert
% Idee -> Address spaces with different sizes, so that slots can be swapped, or address spaces can be merged


% -------------------------------------------------------------------------------------------------
%                                             Cost Analysis
% -------------------------------------------------------------------------------------------------
\section{Cost Analysis}

% A naive approach to performance measurement?
A \textbf{naive approach} to measuring the performance of the virtual memory system is to implement a xv6 user-level program that will result in a number of TLB misses and thus prompt the triggering of the TLB refill mechanism. The execution time of that program can be measured and the run first with the QEMU-emulated MMU, the software page table walker in the TLB miss handler, and the mapping function.

This gives only weak indication of the memory systems actual performance, because the runtime of the operating system running on the emulator strongly depends on the current load of the system running on the host. Additionally, QEMU is an emulator build for performance but not for precise simulation of the hardware \cite{bellard2005qemu}. Especially for this work, a quantitative measurement approach would require accurate measurements of the time it takes the MMU to walk the page table. This is not possible with an emulator, as the high degree of parallelism on which hardware components run can not easily be emulated in a performant way \cite{chen2010emulator}. Thus, the following will focus on a qualitative reasoning of the performance.

% Memory access cost
In the implementation, the cost introduced by \textbf{memory accesses} is very high, because the whole state of the register file is saved to memory. That is an unacceptable cost for a operation that is on the critical path of every TLB miss. It is however possible to omit most of these memory accesses in favor of a light-weight TLB miss handler similar to the fast TLB miss handler in \cite{heiserAnatomyHighPerformanceMicrokernel}. The main difference being that the TLB miss handler in \cite{heiserAnatomyHighPerformanceMicrokernel} performs a page table walk and has thus access main memory. An optimized implementation of the mapping function can work with only a small number of register.

% Reserving registers
To omit all memory accesses from the handler, some registers may be reserved to be only used by kernel code, as MIPS does it with two registers. However, compilers would have to be made aware of that. Depending on the application, reserving registers may still result in a loss of performance because the availability of registers is imperative to the compilers ability to optimize code \cite{elphinstone2013l3}.

% Impact on pipeline
Even when all memory references are eliminated from the handler code, it still impacts both the pipeline and the reorder buffer \cite{jacobSoftwaremanagedAddressTranslation1997}. This can not happen with a hardware-based TLB refill mechanism, as it would typically freeze the pipeline and not touch the reorder buffer \cite{bhattacharjee2017architectural}. Instructions independent from the memory access could still continue execution while the TLB miss is being serviced \cite{jacob1998virtualissues}.

% Inline interrupt handling
\cite{jaleel2001line} proposes a solution that promises to alleviate the cost related to the calling of software interrupt handlers that disturb the pipeline. The paper describes a method to integrate exception handlers directly into the reorder buffer, avoiding the need to flush the pipeline when handling a precise interrupt. The exception handler for TLB misses using a mapping function can be very small and may thus benefit from the idea.




% -------------------------------------------------------------------------------------------------
%                                        Implementation Shortcommings
% -------------------------------------------------------------------------------------------------

\section{Potential Implementation Improvements}

% Flushing not all mappings
\paragraph{TLB Flushing} xv6 always flushes the complete TLB on process switch \cite{cox2011xv6}. However, when switching from a process with ASID n to a process with ASID m, it suffices to flush the TLB entries belonging to ASID n only. The \texttt{sfence.vma} instruction already allows for such fine-grained control of the TLB. The memory fencing effect of the instruction would not change that way \cite{RISCVInstructionSet}.
% Finer TLB Control

\paragraph{Finer TLB Control} As shown in chapter \ref{chap:theory}, MIPS \cite{MIPSArchitectureProgrammers2016} provides a very fine control of its TLB entries. Such fine control would allow the TLB handler to implement custom replacement policies which could adapt the system to current work loads and biasing cache replacement \cite{park2022every}.


%
% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------


% -------------------------------------------------------------------------------------------------


% -------------------------------------------------------------------------------------------------

%
% -------------------------------------------------------------------------------------------------


% vs demand paging





