\chapter{Implementation}

\label{chap:impl}
The implementation consists of two fundamental parts:
The extension of RISC-V ISA emulated by Qemu and the implementation of a
TLB Miss Handler.



\section{Qemu}
This section describes the changes introduced to the Qemu emulator.
It will start with a short overview of the parts of the code relevant for TLB lookups,
TLB management, exceptions and CSRs. Since the implementation runs on a RISC-V
the focus will be on the source code for the qemu RISC-V softmmu target which includes
emulation of a memory management unit.

\todo{Maybe it is better to not have one block for qemu and one block for xv6, because the development
    was pretty interleaved in the beginning}

\todo{Add references to documentation at appropiate places}
\todo{ add disclaimer, that most of this information was derived from reading the source code and may in some places not be accurate because the code is pretty complex and
    i have only been at it for a few months}
\subsection{A short overview of the Qemu source code}
\texttt{cputlb.c} found in \texttt{accel/tcg} contains the TLB-handling logic for all
emulation targets; from there, target-specific functions for TLB management are called.
The target-specific functions are implemented in \texttt{target/riscv}.\\
Every target needs to implement the functions in the \texttt{TCGCPUOps} struct in
\texttt{include/hw/core/tcg-cpu-ops.h}. This struct is part of the glue that connects the target-independent
Tiny Code Generator (TCG) backend with the target-specific functions and structures.

\todo{explain PTE}
\subsection{Adding the TLB MISS exception}
The required behavior for the TLB Miss exception is similar to the behavior of a page fault exception.
The RISC-V Instruction Set Manual: Volume II \cite{RISCVInstructionSetII} describes when page faults
are triggered in the virtual address translation process. Page faults are thrown when:
\begin{itemize}
    \item the valid bit of a PTE is 0; the read bit is 0 and the write bit is 1; any of the reserved
          bits is set
    \item the PTE at the last level of the page table tree does not the read or execute bits set.
    \item the requested memory access type is not allowed by the read, write, execute and user bits.
    \item the PTE describes a missaligned superpage.
\end{itemize}
Depending on active extensions, there are possibly more scenarios where page faults are triggered.\\
The page fault will then make the operating system jump to a specific location in the kernel code
specified by the (physical) address found in the \texttt{mtvec} register.\\
The TLB Miss exception does not have to trigger in similar patterns as the page fault exception, but rather
before the page table is traversed by the emulated hardware pagetable-walker.
However, the code for the page fault exceptions gives hints for where changes need to be made in the Qemu
source to add a custom exception.\\
The \texttt{TCGCPUOps} struct includes a function pointer named \texttt{tlb\_fill}. The function is called
when MMU lookups to the TLB and Qemu's victim TLB fail.
For the riscv target, the function \texttt{riscv\_cpu\_tlb\_fill} is assigned to the \texttt{tlb\_fill}
function pointer in \texttt{target/riscv/tcg/tcg-cpu.c}.\\
The implementation for \texttt{riscv\_cpu\_tlb\_fill} is in \texttt{target/riscv/cpu\_helper.c}. This is a common
file to be found in a target-specifc directory and contains general functionality to implement the \texttt{TCGCPUOps} struct.\\

%TODO Description of the TLB Exception implementation and how to generally add exceptions to the qemu plattform
% TODO List of code locations where changes need to be added -> should be usable as back reference for implementing more exceptions

% TODO discussion and reference to READER/SPECIFICATION on what exception numbers and other constants can be used and are free
%TODO Zwischenschritt: TLB_exception only for special case
Changing the whole system to start throwing a TLB\_MISS Exception on every virtual address would make it very hard to debug both
first tries at implementing a handler for the exception and the exception throwing code in Qemu itself.
And not only would the exception be thrown as soon as virtual memory is activated in \texttt{xv6-riscv:kernel/main.c}, the exception would be thrown
as soon as exceptions are activated and a memory access happens, because Qemu also uses the \texttt{fill\_tlb} routine to fill the TLB with direct
virtual-to-physical mappings when no virtual memory is used. This speeds up the execution of the dynamically-translated code, as it can directly
lookup addresses in the TLB using a fast path.\\
To properly test the implementation, the \texttt{tlb\_fill} function was replaced to throw the TLB\_MISS exception for
one specified, page-aligned address and to continue on normally for every other address. The implementation is outlined
in Listing \ref{lst:specialCaseTLBfill}.

\begin{lstlisting}[language=c,float=t,
    caption={Alternative Implementation for the RISC-V tlb\_fill function with a special case to start testing TLB Miss Handler implementations},
    label={lst:specialCaseTLBfill}]
bool my_riscv_cpu_tlb_fill(CPUState *cs, vaddr address, int size,
    MMUAccessType access_type, int mmu_idx,
    bool probe, uintptr_t retaddr)
{
    RISCVCPU *cpu = RISCV_CPU(cs);
    CPURISCVState *env = &cpu->env;
    int mode = mmuidx_priv(mmu_idx);
    int vm = get_field(env->satp, SATP64_MODE);
    bool ret = false;

    if(!(vm == VM_1_10_MBARE || mode == PRV_M) &&
            address == (uint64_t)0x84fff000) {
        ret = riscv_cpu_tlb_miss_exception(cs,address,size,access_type, mmu_idx, probe, retaddr);
    } else {
        ret =  riscv_cpu_tlb_fill(cs,address,size,access_type, mmu_idx, probe, retaddr);
    }
    return ret;
}
\end{lstlisting}

%TODO Explain what the riscv_cpu_tlb_fill function does originally - in detail

\todo{sequence diagram for function calls accross the qemu repository}
%TODO Custom TLB Exception based off of the page fault exception
\paragraph{Adding a new exception}to the Qemu emulator requires changes at a number of places. In the following,
the relevant code locations in the Qemu source are shown.\\
This may be completely different for other targets, as the exception code is mostly target specific and
this implementation only looked at the RISC-V target.
\begin{itemize}
    \item \texttt{target/riscv/cpu\_bits.h} contains all CPU-definitions specific to the RISC-V target.
          There is also a enum called \texttt{RISCVException} which contains the number-codes for all RISC-V exceptions.
          In choosing a appropiate number for a new exception, one should consult the Privileged Architecture Specification \cite{RISCVInstructionSetII}.
          There are specific exception code ranges that are designated for custom use. E.g. the codes 24--32 and 48--63.
    \item \texttt{target/riscv/cpu\_helper.c\:riscv\_cpu\_do\_interrupt\(\)} is the target-specific function
          for triggering interrupts. Here it suffices to add the new exception enum item to the switch case, when
          the new exception is similar in behavior to exising exceptions.\\
          Here the new exception is simply supposed to jump into an exception handler in the kernel. A lot of exceptions
          like page faults share that behavior.
    \item Finally, if the exception should be delegatable to supervisor mode or user mode, the n-th bit,
          with n being the exception code, needs to be set in the \texttt{DELEGABLE\_EXCPS} definition in \texttt{target/riscv/csr.c}.
          This enables the kernel to delegate the exception to another priviledge level by setting the appropiate
          bit in the \texttt{medeleg} and \texttt{sedeleg} CSRs.
\end{itemize}
\todo{what is a csr?}
%TODO Elabortation on the TLB Fast Path
\subsection{Adding new CSRs for writing TLB entries}

\subsection{Extending the Qemu Monitor}


\todo{move to discussion of implementation at the end of this chapter}
\section{Further possible extensions to the TLB CSRs}
With the new CSRs presented above, it is possible to write TLB entries.
The placement of the entry and thus the entry to be replaced can not be selected.
The replacement policy would be completely up to the hardware implementation.
\todo{typical tlb replacement strategies}
\todo{How does qemu replace TLB entries -> index?}
It would also be possible to further extend the ISA with additional CSRs to
support specific selection of entries to be replaced, or to select between
a number of replacement strategies.
However, the efficiency of more complex strategies need to be weighed carefully
against their benefits, since the TLB filling is on the critical path of all
memory accesses.

% \section{xv6}
% \subsection*{}
% \subsection*{}






% \section{Platform}
% \subsection{Platform Considerations}
% % Which aspects are important for chosing the platform for this project?
% % Why have I not chosen L4, Mips, etc? They already have some preconditions that are pretty good for my work (software handler, tlb instructions)
% \subsection{xv6}
% \subsection{RISC-V}
% \subsection{Qemu}
% \paragraph{softmmu Target}




% \section{Relevant Modules in the xv6 Source}
% \paragraph*{kalloc - physical memory allocator}
% \paragraph*{proc}
% \paragraph*{vm.c}
% \subsection*{Syscalls to change}
% System call interface should stay the same so that existing processes can continue
% as usual -> new virtual memory scheme should be completely transparent%
%\paragraph*{fork}
% creating a new addresse space -> fixed number of processes ? ?
%\paragraph*{exit}
% freeing memory?
%\paragraph*{exec}
% same as fork?
%\paragraph*{sbrk}
% allocating new memory
Sbrk is the system call with which a program can change its program break. The program break
is the end of the programs data segment. This system call is typically used to either
increase or decrease a programs memory.
Sbrk would typically be called by the memory manager of the user program. Calls
to malloc would for example call sbrk when there is not enough memory available to
satisfy the call to malloc.
Internally, sbrk calls a function called growproc(n) which grows or shrinks to calling
processes memory by n bytes.
growproc will then call either uvmalloc() or uvmdealloc(). These functions need to be
changed to use the new allocation scheme.

\paragraph*{Experimental Allocation Scheme}
xv6 keeps all of its unused physical memory in a linked list. The linked list itself is stored
in the free blocks, as they are not used for anything else. When a process asks for more memory
the kalloc() function retrieves the first element from the linked list, updates the head
of the list to point to the next element and returns the pointer to the first element,
which is the same as the physical address for the page managed by that first list item.
kalloc() is called by the uvmalloc() function, and since we want to change the way
the virtual memory management works, it would make sense to directly change the uvmalloc() function.
However, in order to properly test the physical memory allocation first, we will change
the allocation scheme first and then the mapping, as the trampoline page would not work anymore.

The new allocation scheme statically assigns a fixed portion of the available physical memory
to each process. The size is determined by the size of physical memory and the maximum number
of processes.
kalloc() will then use the process id, the current size of the process and the requested
increase in size to determine the next page for the process.

\subsection*{Why does this not work for anything else but the sbrk syscall}
- kalloc with sbrk uses the current process size to determine the next free space in
the address space of the process
- The processes size only keeps track of the text data and so on segments. So only stuff
the program is aware of. Page table pages need to be allocated per process as well but
do not contribute to the overall size of the process so they can not be put into the
processes memory space. This has the direct advantage that users can not access their page tables
but this was never the problem because pages containing page tables could simply be marked
as not user accessible. And of course this brings some disadvantages: Kernel size becomes
a scarce resource and user address spaces are not self contained anymore, page tables now reside
somewhere else entirely.
But since it is the goal to get rid of page tables anyways, its not too big of a problem.
When we get rid of the page tables, we get rid of a huge part of the tables that can not
be directly allocated using the respective process size.
What are the other pages and allocations that can not rely on the processes size?

One further problem is that deallocation is not working. But in principle, all pages belonging
strictly to one process can be allocated on this sort of stack. The size counter
needs to be adapted to not only account for the processes own memory, but also
for all the memory that is allocated for the process.
% \section{Changing physical memory allocation}
% \subsection{Special cases}
% \paragraph*{Kernel direct mapping}
% \paragraph*{MMIO}
% \paragraph*{Trampoline}
% \section{Modifying Qemu Source}
% \subsection{TLB Miss Exception}
% \subsection{CSR Modifications}
% \subsubsection{TLB Modification via CSR}
% \subsubsection{Switching between HW and SW TLB miss handling}

% \section{xv6 TLB Miss Handler implementation Idea}
% \subsection{code outline}
% \subsection{Simplest case}
% \subsection{Extrapolation target, most complex case}
% \subsection{features to be implemented}
% \subsection{realisation - how far have I come}

% \section{Implementation description}
% \subsection{Problems}
% \subsection{idiosyncracies}
% \subsection{limitations}

%\section{Veriying the Mapping - Mapping Tester}
\section{Challange: Modifying code in parts}
% in order to test single changes, changes should be isolated to a specific
% kernel module and not change the interface between modules
\subsection*{Changing the physical memory allocation}
% each process gets a fixed set of physical pages assigned to it
% TODO How much processes does xv6 run at a time? -> Least amount of processess to have 
% the operation continue as usual

\section{Simplest Case: Fixed memory area for each process}
\subsection{Address Spaces}
The available physical memory is split into evenly sized portions, from now called address space.
When a new process is started it tries to acquire an address space by checking whether
there is a free one and then claiming that free address space.
The number of address spaces will thus limit the number of concurrently running programs.
Why not just use the Pids and reuse them? The order of termination is not clear, bookkeeping
may be expensive. PID range is typically a lot bigger -> But this can be limited.
\subsection{Discussion: Limitations}
This is not really virtual memory and works only, because xv6 just added more memory at the low
end of the address space. No mappings are made in between or at high addresses.
This is where a hash function could come in handy. However collisions and access rights and
collision detection \ldots
Verification via % \texttt{\verbatim{readelf -l user/\_*}}
shows that all vaddr are in the low addresses.
\paragraph*{Fragmentation}
%Processes have fixed size of memory and may not use all of it
\paragraph*{Limited Process count}
%Maybe appropiate for embedded applications?




\section{Debugging}
\subsection{ GDB - custom satps}